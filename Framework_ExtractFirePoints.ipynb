{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1188fcf8",
   "metadata": {},
   "source": [
    "# Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed211577",
   "metadata": {},
   "source": [
    "It should be noted that the code provided in this Jupyter notebook is not perfect, as its primary purpose is to demonstrate the feasibility of the proposed framework presented in the paper titled\n",
    "\n",
    "\"Framework to Create Inventory Dataset for Disaster Behavior Analysis Using Google Earth Engine: A Case Study in Peninsular Malaysia for Historical Forest Fire Behavior Analysis\"\n",
    "\n",
    "Forests 2024, 15(6), 923; https://doi.org/10.3390/f15060923\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fd469",
   "metadata": {},
   "source": [
    "# Prerequisite - Installing Geemap locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081bc9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEE Installation through Conda \n",
    "# conda create -n gee geopandas ipykernel python=3.9\n",
    "# conda install -n base nb_conda_kernels\n",
    "# conda install -n base conda-libmamba-solver\n",
    "# conda config --set solver libmamba\n",
    "# conda install -c conda-forge geemap localtileserver\n",
    "# jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902c2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade geemap to the latest version\n",
    "# geemap.update_package()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71644739",
   "metadata": {},
   "source": [
    "# Creating Single Polygon from Feature Collection to reduce computation\n",
    "- Country Boundary (all_states) - level 1 can be downloaded from: https://data.humdata.org/dataset/cod-ab-mys\n",
    "- Modify this section to apply the framework to other location\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78764128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import os\n",
    "import pandas as pd\n",
    "from geemap.datasets import DATA, get_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bcc3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec59d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Adding Malaysia States\n",
    "all_states = ee.FeatureCollection(\"projects/chewyeejian/assets/mys_adm2\")\n",
    "\n",
    "\n",
    "#  _____Location Selection in Malaysia  via Feature Collection_____\n",
    "#  [ADM1_PCODE] = MY01-Johor | MY02-Kedah | MY03-Kelantan | MY04-W.P. Kuala Lumpur | MY06-Melaka | MY07-Negeri | MY08-Pahang | MY09-Perak | MY10-Perlis | MY11-Pulau Pinang | MY15-Terengganu | MY16-W.P. Putrajaya | MY17-Selangor\n",
    "#  [ADM1_PCODE] = MY05-W.P. Labuan | MY12-Sabah | MY13-Sarawak\n",
    "peninsular = all_states.filter(ee.Filter.Or(\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY01\"),ee.Filter.eq('ADM1_PCODE',\"MY02\"),ee.Filter.eq('ADM1_PCODE',\"MY03\"),\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY04\"),ee.Filter.eq('ADM1_PCODE',\"MY06\"),ee.Filter.eq('ADM1_PCODE',\"MY07\"),\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY08\"),ee.Filter.eq('ADM1_PCODE',\"MY09\"),ee.Filter.eq('ADM1_PCODE',\"MY10\"),                                    \n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY11\"),ee.Filter.eq('ADM1_PCODE',\"MY14\"),ee.Filter.eq('ADM1_PCODE',\"MY15\"),\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY16\"),ee.Filter.eq('ADM1_PCODE',\"MY17\")                                    \n",
    "                                    ))\n",
    "\n",
    "sabahsarawak_states = all_states.filter(ee.Filter.Or(\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY05\"),ee.Filter.eq('ADM1_PCODE',\"MY13\"),ee.Filter.eq('ADM1_PCODE',\"MY12\")))\n",
    "\n",
    "pahang_states = all_states.filter(ee.Filter.Or\n",
    "                                  (ee.Filter.eq('ADM1_PCODE',\"MY08\")))\n",
    "                                    \n",
    "# Map.addLayer(all_states,{},\"All States\")\n",
    "# Map.addLayer(peninsular,{},\"Peninsular\")\n",
    "# Map.addLayer(sabahsarawak_states,{},\"Sabah-Sarawak\")\n",
    "\n",
    "# Use the international boundary\n",
    "# Malaysia = ee.FeatureCollection(\"USDOS/LSIB/2017\").filterMetadata(\"COUNTRY_NA\",\"equals\",\"Malaysia\")\n",
    "# Malaysia = ee.FeatureCollection(\"USDOS/LSIB/2017\").filterMetadata(\"COUNTRY_NA\",\"equals\",\"Australia\")\n",
    "# Map.addLayer(Malaysia,{},\"malaysia\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb74b19",
   "metadata": {},
   "source": [
    "## Parameters to Set - Location & Dates\n",
    "- year set to 2021 year (coz we got rompin case study)\n",
    "- location set to pahang first (before we scale to the entire peninsular) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e5f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  _____Location Selection in Malaysia  via Feature Collection_____\n",
    "feature_selected_states = peninsular #multipolygon\n",
    "# selected_states = peninsular #not used\n",
    "# print(feature_selected_states,'feature_selected_states')\n",
    "Map.addLayer(feature_selected_states,{'color':'808080'},\"feature_selected_states\")\n",
    "Map.centerObject(feature_selected_states,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aca9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_start = ee.Date(\"2001-01-01\")\n",
    "fire_end   = ee.Date(\"2024-01-01\")\n",
    "\n",
    "# # for testing purpose, we set to 6 month and pahang only \n",
    "# fire_start = ee.Date(\"2015-01-01\")\n",
    "# fire_end   = ee.Date(\"2016-01-01\")\n",
    "# feature_selected_states = pahang_states #multipolygon\n",
    "# selected_states = pahang_states #single polygon to reduce computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73f155",
   "metadata": {},
   "source": [
    "# Historical Points Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3486fd",
   "metadata": {},
   "source": [
    "## Fire Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d21768",
   "metadata": {},
   "source": [
    "### MCd64A1 MODIS Burned Area Monthly Global 500m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c435fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_MCD64A1 = ee.ImageCollection('MODIS/061/MCD64A1').select('BurnDate').filterDate(fire_start, fire_end)\n",
    "dataset_MCD64A1 = dataset_MCD64A1.map(lambda image:image.clip(feature_selected_states))\n",
    "\n",
    "burnedAreaVis = {\n",
    "  'min': 0.0,\n",
    "  'max': 366.0,\n",
    "  'palette': ['4e0400', '951003', 'c61503', 'ff1901'],\n",
    "}\n",
    "\n",
    "\n",
    "Map.setCenter(102.206, 3.744, 7)\n",
    "Map.addLayer(dataset_MCD64A1, burnedAreaVis, 'Burned Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569f391",
   "metadata": {},
   "source": [
    "### Sample Region - Extracting Fire Points from the MCD64A1 Area\n",
    "- Extract all the coordinates points from the burnt area region\n",
    "- Export the location as .csv - Yes (Actually is it necessary? We can just process the whole script, but will it crash?)\n",
    "- Because we also need to find out the location of non-fire (pending) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e15c8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each of the image in image collection to get the points out \n",
    "def rasterExtraction(image):\n",
    "    feature = image.sampleRegions(\n",
    "        collection = feature_selected_states, # feature collection here\n",
    "        scale = 1000, # Cell size of raster\n",
    "        geometries = True\n",
    "    )\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddd3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this part to select more parameters to obtain from the image collection \n",
    "fire_points = dataset_MCD64A1.filterBounds(feature_selected_states).select('BurnDate').map(rasterExtraction).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4361d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the label (fire = 1) indicating fire in the .csv \n",
    "def add_properties_fire(feature):\n",
    "    # Get the geometry of the feature (point)\n",
    "    point = ee.Geometry(feature.geometry())\n",
    "    # Extract latitude and longitude from the point geometry\n",
    "    latitude = point.coordinates().get(1)\n",
    "    longitude = point.coordinates().get(0)\n",
    "    properties = {'fire':1, 'latitude': latitude, 'longitude': longitude}\n",
    "    return feature.set(properties)\n",
    "fire_points = fire_points.map(add_properties_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6d54153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fire_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148a04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Map.addLayer(fire_points, {'color': 'purple'}, 'FireRegionPoints', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7d2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36ff54",
   "metadata": {},
   "source": [
    "## Non-Fire Point Extraction\n",
    "- All available burnt area data within the region of interest from the MCD64A1 dataset is utilized.\n",
    "- To improve the confidence of the the non-fire points,the FIRMS hotspots dataset is also included.\n",
    "- The burnt areas and FIRMS hotspots are blended into a single image, depicting the historical burnt regions across all years. \n",
    "- An additional dilation morphological operation is applied to expand the boundaries of the burnt regions and hotspots, with the default radius and iteration value set to 2. \n",
    "- To obtain the non-fire regions, we invert the selection of burnt area region with the region of interest. \n",
    "- the GEE function ee.FeatureCollectionRandomPoints is employed to randomly extract non-fire points at a 1kmÂ² resolution, \n",
    "- with the total number of non-fire points matching the total number of fire points. \n",
    "- For the month and day of the fire for non-fire points, we can leverage the most recent available year in the MCD64A1 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c95a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 10 years firm dataset \n",
    "# dataset_firms = ee.ImageCollection('FIRMS').select('T21').filterDate(fire_start, fire_end)\n",
    "dataset_firms = ee.ImageCollection('FIRMS').select('T21').filterDate(ee.Date(\"2014-01-01\"), ee.Date(\"2024-01-01\"))\n",
    "\n",
    "# Clip dataset to selected area by using feature collection - ensure any hotspots outside of the selected area is not included\n",
    "dataset_firms = dataset_firms.map(lambda image:image.clip(feature_selected_states))\n",
    "\n",
    "# FIRMS_Visualization =  {'min': 1,\n",
    "#                           'max': 100,\n",
    "#                           'palette': [\"white\", \"yellow\", \"orange\", \"red\"]}\n",
    "\n",
    "\n",
    "# # Add the firms to the maps\n",
    "# Map.addLayer(dataset_firms, FIRMS_Visualization, '20 Years Accumulated Hotspot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b77938ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Image collection --> Image with the total counts of fire hotspots\n",
    "dataset_firms_count = dataset_firms.count() \n",
    "\n",
    "# Convert Fire Counts to binary (0 or 1) \n",
    "dataset_firms_binary = dataset_firms_count.eq(dataset_firms_count).rename('FIRMS_binary')\n",
    "\n",
    "FIRMS_Visualization1 =  {'min': 0,\n",
    "                          'max': 1,\n",
    "                          'palette': [\"green\", \"red\"]}\n",
    "Map.addLayer(dataset_firms_binary, FIRMS_Visualization1, 'firms_fire_binary',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002d3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the burnt area dataset MCD64A1, combine with the firms dataset \n",
    "dataset_MCD64A1_count = dataset_MCD64A1.count()\n",
    "dataset_MCD64A1_binary = dataset_MCD64A1_count.eq(dataset_MCD64A1_count).rename('FIRMS_binary')\n",
    "Map.addLayer(dataset_MCD64A1_binary, FIRMS_Visualization1, 'dataset_MCD4A1_binary',0)\n",
    "\n",
    "# .blend will overlays one image on top of another\n",
    "\n",
    "merged_fire_binary = dataset_firms_binary.blend(dataset_MCD64A1_binary)\n",
    "Map.addLayer(merged_fire_binary, FIRMS_Visualization1, 'merged_fire_binary',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba03acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a dilation with radius of 1 and iterations of 1 to increase the coverage area of FIRMS hotspots\n",
    "# May change the iterations value to increase the size of firms hotspots area \n",
    "# Performing this to avoid the random points from taking any missed area of fire hotspots \n",
    "# By dilating, we can also reduce the number of features \n",
    "kernel = ee.Kernel.circle(**{'radius': 2})\n",
    "\n",
    "# Perform a dilation, display.\n",
    "merged_fire_binary = merged_fire_binary \\\n",
    "             .focalMax(**{'kernel': kernel, 'iterations': 2})\n",
    "\n",
    "Map.addLayer(merged_fire_binary, FIRMS_Visualization1, 'merged_fire_dilation',0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ecdfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the fire points to 0 \n",
    "dataset_nofire = merged_fire_binary.eq(0)\n",
    "\n",
    "# Create a mask by changing all the null values to -1 to show all the non-fire region \n",
    "dataset_nofire_mask = dataset_nofire.unmask(-1).clip(feature_selected_states)\n",
    "\n",
    "# Update the mask by removing non fire region from the dataset \n",
    "dataset_nofire = dataset_nofire_mask.updateMask(dataset_nofire_mask.eq(-1))\n",
    "Map.addLayer(dataset_nofire, FIRMS_Visualization1, 'firms_nofire',0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12f650ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling fire_points with more than 5000 points will result in unable to call\n",
    "# may filter if want to call accordingly \n",
    "# fire_points.filter(ee.Filter.lt('BurnDate',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad5479ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the binary firms to vectors \n",
    "# Using a too large scale will caused smaller pixels to be missed \n",
    "dataset_nofire_vector = dataset_nofire.reduceToVectors(\n",
    "    **{\n",
    "  'geometry': feature_selected_states,\n",
    "  'scale': 1000,\n",
    "  'geometryType': 'polygon',\n",
    "  'eightConnected': False,})\n",
    "Map.addLayer(dataset_nofire_vector.draw(**{'color': 'green', 'strokeWidth': 1}),{},'FIRMS No-Fire Area Vectorized', 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c041d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the same number points (as the number of firepoints) from the no_fire_vector\n",
    "# Might take awile to complete the feature collection --> size (number) \n",
    "number_fire_points = fire_points.size()\n",
    "nofire_points = ee.FeatureCollection.randomPoints(dataset_nofire_vector,number_fire_points)\n",
    "\n",
    "# When the random points is sample, it might sample the boundaries of the feature_selected_states\n",
    "# Thus, the filterBounds will be performed again to remove at the points of the boundary of the feature_selected_states\n",
    "# Notes: 25/07/2023 - The codes will run into error if the points is at the boundary of the feature_selected_states \n",
    "nofire_points  = nofire_points.filterBounds(feature_selected_states)\n",
    "Map.addLayer(nofire_points,{'color':'green', 'pointRadius': 1,'strokeWidth': 1},'No-Fire Random Points', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "212106eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_properties(feature):\n",
    "    point = feature.geometry()\n",
    "    \n",
    "    # Find the properties from the property collection that correspond to the point\n",
    "    properties = feature_selected_states.filterBounds(point).first().toDictionary()\n",
    "    \n",
    "    # Merge the existing properties of the point with the properties from the property collection\n",
    "    return feature.set(properties)\n",
    "    \n",
    "\n",
    "# Add the label (fire = 0) indicating no fire in the .csv \n",
    "# Add the burndate as -1\n",
    "# Add the latitude and longtiude \n",
    "def add_properties_nofire(feature):\n",
    "    # Get the geometry of the feature (point)\n",
    "    point = ee.Geometry(feature.geometry())\n",
    "    # Extract latitude and longitude from the point geometry\n",
    "    latitude = point.coordinates().get(1)\n",
    "    longitude = point.coordinates().get(0)\n",
    "    properties = {'fire':0, 'BurnDate':-1, 'latitude': latitude, 'longitude': longitude}\n",
    "    return feature.set(properties)\n",
    "\n",
    "nofire_points = nofire_points.map(get_properties)\n",
    "nofire_points = nofire_points.map(add_properties_nofire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38babc09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda536b0",
   "metadata": {},
   "source": [
    "## Merge Fire and No-Fire Points & Export as .csv\n",
    "- combined both the points from fire and non fire\n",
    "- use ee_to_csv to export the points to .csv file \n",
    "- recommended to export and save to avoid stressing GEE / crashing \n",
    "- easier to load back if anything crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b564c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____Merge Training Points______________________________-\n",
    "allfire_combinedpoints = fire_points.merge(nofire_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51ff9c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/4b8460ec9767e6ec016eabd5b68a52a5-9537492d044ec25b72d51af05bb2d400:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\fire_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the fire & non-fire points to .csv in the same directory with name fire_dataset.csv \n",
    "# Might take some time depending on the number of points\n",
    "geemap.ee_to_csv(allfire_combinedpoints, filename='fire_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7134cd",
   "metadata": {},
   "source": [
    "## Extract the Years / Months / Day from the Fire_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38f6a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fire_dataset = geemap.csv_to_df('fire_dataset.csv')\n",
    "\n",
    "### Extract the Year Month and Day from system:index\n",
    "### Extract day from BurnDate\n",
    "### Replace non burn date with 31-12-2022\n",
    "\n",
    "df_fire_dataset[['year', 'month']] = df_fire_dataset['system:index'].str.extract(r'_(\\d{4})_(\\d{2})_')\n",
    "df_fire_dataset['year'].fillna('2023', inplace=True)\n",
    "df_fire_dataset['month'].fillna('12', inplace=True)\n",
    "\n",
    "# Replace -1 with 1 in the 'BurnDate' for 'day' column\n",
    "df_fire_dataset['day'] = df_fire_dataset['BurnDate'].replace(-1, 31)\n",
    "# Convert the day of year to the date of the month\n",
    "df_fire_dataset['day'] = pd.to_datetime(df_fire_dataset['day'].astype(str) + ' ' + df_fire_dataset['year'].astype(str), format='%j %Y').dt.day\n",
    "\n",
    "### Set Index\n",
    "df_fire_dataset.set_index('system:index', inplace=True)\n",
    "\n",
    "### Export fire_dataset\n",
    "df_fire_dataset.to_csv(\"fire_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2d9a9",
   "metadata": {},
   "source": [
    "# CSV Import back to Earth Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf8275",
   "metadata": {},
   "source": [
    "## Additional Module, Split to fire_points.csv and nofire_points.csv for Visualization in QGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2cb5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fire_dataset_export = df_fire_dataset[df_fire_dataset['fire']==1]\n",
    "df_nofire_dataset_export = df_fire_dataset[df_fire_dataset['fire']==0]\n",
    "\n",
    "df_fire_dataset_export.to_csv('fire_points_only.csv')\n",
    "df_nofire_dataset_export.to_csv('nofire_points_only.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a55637",
   "metadata": {},
   "source": [
    "## Loading Fire Points Dataset \n",
    "- load fire points from the .csv back to gee \n",
    "- do not recommend to do all in one shot as GEE - out of memory / exceed resource\n",
    "- Extract Year / Month / Day of Fire \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e03f3bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map is defined, do not replace!\n"
     ]
    }
   ],
   "source": [
    "### Initialized ee , defined Map if haven't \n",
    "import ee\n",
    "import geemap\n",
    "import os\n",
    "from geemap.datasets import DATA, get_metadata\n",
    "\n",
    "# If the codes run from top, calling Map = geemap.Map() will remove all the previous layers \n",
    "try:\n",
    "    Map\n",
    "except NameError:\n",
    "    print(\"Map not defined, define now!\")\n",
    "    Map = geemap.Map()\n",
    "else:\n",
    "    print(\"Map is defined, do not replace!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3efde9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import geopandas\n",
    "from pandas import json_normalize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da9ae4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fire points = 5650\n",
      "number of no_fire points = 5629\n"
     ]
    }
   ],
   "source": [
    "### Load the fire_dataset.csv from the same directory as df \n",
    "df_fire_dataset = geemap.csv_to_df('fire_dataset.csv')\n",
    "\n",
    "### Print out the number of fire points and non-fire points \n",
    "print(\"number of fire points =\" , len(df_fire_dataset[df_fire_dataset['fire'] == 1]))\n",
    "print(\"number of no_fire points =\" , len(df_fire_dataset[df_fire_dataset['fire'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01d4aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop all columns containing NaN values as GEE cannot accept full NaN column \n",
    "df_fire_dataset = df_fire_dataset.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1d5a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter the fire / no-fire points (some points) for smaller sets testing (use only 100 points)\n",
    "### Comment the lines below if want to use the full set \n",
    "# df_fire_dataset = df_fire_dataset[df_fire_dataset['ADM1_PCODE']==\"MY08\"].sample(n=100)\n",
    "# print(\"number of fire points =\" , len(df_fire_dataset[df_fire_dataset['fire'] == 1]))\n",
    "# print(\"number of no_fire points =\" , len(df_fire_dataset[df_fire_dataset['fire'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b797ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert pandaframes to EE feature collection\n",
    "fire_dataset_points = geemap.pandas_to_ee(df_fire_dataset, latitude='latitude',longitude='longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21fc057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fire points = 5650\n",
      "number of no_fire points = 5629\n"
     ]
    }
   ],
   "source": [
    "print(\"number of fire points =\" , len(df_fire_dataset[df_fire_dataset['fire'] == 1]))\n",
    "print(\"number of no_fire points =\" , len(df_fire_dataset[df_fire_dataset['fire'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73f92e",
   "metadata": {},
   "source": [
    "## Visualizing the Fire Points Dataset\n",
    "- For sample visualization only (< 1000 points)\n",
    "- use qgis if want to visualize all 10,000 points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ebe0e83c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fire_dataset_points_map = fire_dataset_points.filter(ee.Filter.eq('fire', 1))\n",
    "nofire_dataset_points_map = fire_dataset_points.filter(ee.Filter.eq('fire', 0))\n",
    "Map.addLayer(fire_dataset_points_map,{'color':'red'}, \"fire_dataset_points_map\")\n",
    "Map.addLayer(nofire_dataset_points_map,{'color':'green'}, \"nofire_dataset_points_map\")\n",
    "# Map.addLayer(fire_dataset_points,{'color':'black'}, \"fire_dataset.csv\", 0)\n",
    "Map.setCenter(102.206, 3.744, 7)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82aa21",
   "metadata": {},
   "source": [
    "## Remove Columns Before Extracting Variables from other Datasets \n",
    "- retain only important columns before extracting variables from GEE\n",
    "- aka - remove all the states features and etc.\n",
    "- reduce the number of features for each of the .csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cba9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column = ['latitude','longitude','fire','system:index','year','month','day']\n",
    "df_fire_dataset_filtered = df_fire_dataset[selected_column]\n",
    "fire_dataset_points = geemap.pandas_to_ee(df_fire_dataset_filtered, latitude='latitude',longitude='longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a52ebd",
   "metadata": {},
   "source": [
    "# Pixels Extraction Notes\n",
    "- Seperate into two section --> Image Collection / Image \n",
    "- For daily variables --> perform monthly aggregation in GEE to reduce the number of features extracted out to avoid resource/memory issues \n",
    "- For Monthly variables --> extract all of them, perform seasonal / annual average in pandas \n",
    "- For Annual variables --> extract all of them \n",
    "- For Monthly/Annual variables --> Create a new column \"current_year_variables\" (linking fire and variables year) \n",
    "- For each source, generate 1 csv files \n",
    "- However, due to GEE limitation, when too many points + variables, it will caused resource/memory issues\n",
    "- Thus, extract each variables one by one, then merge all of them together \n",
    "- Once all the source have been extracted, merged the .csv using pandas (by matching the system:index with the original fire_dataset.csv) \n",
    "- The full dataset is now ready to be analysed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d143ef8",
   "metadata": {},
   "source": [
    "## Declaring States and Fire start and end Variables\n",
    "- Rerun to declare the feature collection of the area of interest in case GEE crashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52c0b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables before extracting\n",
    "#Adding Malaysia States\n",
    "all_states = ee.FeatureCollection(\"projects/chewyeejian/assets/mys_adm2\")\n",
    "\n",
    "#  _____Location Selection in Malaysia  via Feature Collection_____\n",
    "#  [ADM1_PCODE] = MY01-Johor | MY02-Kedah | MY03-Kelantan | MY04-W.P. Kuala Lumpur | MY06-Melaka | MY07-Negeri | MY08-Pahang | MY09-Perak | MY10-Perlis | MY11-Pulau Pinang | MY15-Terengganu | MY16-W.P. Putrajaya | MY17-Selangor\n",
    "#  [ADM1_PCODE] = MY05-W.P. Labuan | MY12-Sabah | MY13-Sarawak\n",
    "peninsular = all_states.filter(ee.Filter.Or(\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY01\"),ee.Filter.eq('ADM1_PCODE',\"MY02\"),ee.Filter.eq('ADM1_PCODE',\"MY03\"),\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY04\"),ee.Filter.eq('ADM1_PCODE',\"MY06\"),ee.Filter.eq('ADM1_PCODE',\"MY07\"),\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY08\"),ee.Filter.eq('ADM1_PCODE',\"MY09\"),ee.Filter.eq('ADM1_PCODE',\"MY10\"),                                    \n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY11\"),ee.Filter.eq('ADM1_PCODE',\"MY14\"),ee.Filter.eq('ADM1_PCODE',\"MY15\"),\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY16\"),ee.Filter.eq('ADM1_PCODE',\"MY17\")                                    \n",
    "                                    ))\n",
    "\n",
    "sabahsarawak_states = all_states.filter(ee.Filter.Or(\n",
    "                                    ee.Filter.eq('ADM1_PCODE',\"MY05\"),ee.Filter.eq('ADM1_PCODE',\"MY13\"),ee.Filter.eq('ADM1_PCODE',\"MY12\")))\n",
    "\n",
    "pahang_states = all_states.filter(ee.Filter.Or\n",
    "                                  (ee.Filter.eq('ADM1_PCODE',\"MY08\")))\n",
    "\n",
    "\n",
    "#  _____Location Selection in Malaysia  via Feature Collection_____\n",
    "feature_selected_states = peninsular #multipolygon\n",
    "selected_states = peninsular #single polygon to reduce computation\n",
    "# print(feature_selected_states,'feature_selected_states')\n",
    "Map.addLayer(feature_selected_states,{'color':'808080'},\"feature_selected_states\")\n",
    "Map.centerObject(feature_selected_states,8)\n",
    "\n",
    "\n",
    "fire_start = ee.Date(\"2001-01-01\")\n",
    "fire_end   = ee.Date(\"2024-01-01\")\n",
    "\n",
    "# # for testing purpose, we set to 6 month and pahang only \n",
    "# fire_start = ee.Date(\"2021-01-01\")\n",
    "# fire_end   = ee.Date(\"2023-01-01\")\n",
    "# feature_selected_states = pahang_states #multipolygon\n",
    "# selected_states = pahang_states #single polygon to reduce computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3909b3f",
   "metadata": {},
   "source": [
    "# Image Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51c84f",
   "metadata": {},
   "source": [
    "## Use pandas to extract yearly and seasonal averages\n",
    "- why we do this? Instead of doing in GEE, extract everything\n",
    "- allow for better data analysis / data scientist to easily work with it\n",
    "- however, when the number of bands is more than 5000, GEE will not allow us to extract\n",
    "- hence, if daily information is provided, we shall extract the monthly average before export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ade87153",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to process the column name to obtain the annual / seasonal values from the current \"date\" of fire  \n",
    "def process_annual_seasonal(df_ori):\n",
    "    ### Set the index to system:index\n",
    "    df_ori.set_index('system:index', inplace=True)\n",
    "    \n",
    "    ### Make a copy of the original df\n",
    "    df = df_ori.copy()\n",
    "    \n",
    "    ### obtain the variable names through the columns names\n",
    "    column_list = df.columns[df.columns.str.contains('_')].str.split('_').str[1].drop_duplicates()  \n",
    "    print(column_list)\n",
    "    \n",
    "    ### loops to loop through each of the variables to obtain the seasonal values and annual average \n",
    "    ## ensure that the variable format is YYYYMM_????\n",
    "    ## annual --> after split with _, select the first element, and take only the first 4 character which is the year\n",
    "    ## seasonal average\n",
    "    for items in column_list:\n",
    "        print(\"calculating annual average and seasonal average for \"+items)\n",
    "        df = df.filter(like=items).groupby(by=lambda x: str(x.split('_')[0][:4]), axis=1).mean().add_suffix('_'+items+'_annual').merge(df, on='system:index', how='outer')\n",
    "        df = df.filter(regex=r'^\\d{4}(12|01|02)_'+items).groupby(by=lambda x: str(x.split('_')[0][:4]), axis=1).mean().add_suffix('_'+items+'_DJF').merge(df, on='system:index', how='outer')\n",
    "        df = df.filter(regex=r'^\\d{4}(03|04|05)_'+items).groupby(by=lambda x: str(x.split('_')[0][:4]), axis=1).mean().add_suffix('_'+items+'_MAM').merge(df, on='system:index', how='outer')\n",
    "        df = df.filter(regex=r'^\\d{4}(06|07|08)_'+items).groupby(by=lambda x: str(x.split('_')[0][:4]), axis=1).mean().add_suffix('_'+items+'_JJA').merge(df, on='system:index', how='outer')\n",
    "        df = df.filter(regex=r'^\\d{4}(09|10|11)_'+items).groupby(by=lambda x: str(x.split('_')[0][:4]), axis=1).mean().add_suffix('_'+items+'_SON').merge(df, on='system:index', how='outer')\n",
    "        \n",
    "    ### Combined the annual/seasonal average with the original df      \n",
    "    df_ori = df_ori.combine_first(df)\n",
    "#         print(df_ori)\n",
    "\n",
    "    ### Create another copy of the df containing the annual/seasonal average \n",
    "    df = df_ori.copy()\n",
    "    \n",
    "    ### Loop through each of the available year in [year] column \n",
    "    ##  Find the current year variables and rename the column name to current \n",
    "    ##  Compare the year in the \"year column\" with the variables (column) name YYYYMM \n",
    "    ##  Combine the extracted columns with the original df \n",
    "    print(\"obtaining the current year average for each of the fire point \")\n",
    "    for year in df_ori['year'].unique():\n",
    "#         print(year)\n",
    "        df = df_ori.copy()\n",
    "        regex_pattern = r'^' + str(year)     \n",
    "        df = df.query('year == @year').filter(regex=regex_pattern)\n",
    "        df.columns = df.columns.str.replace(r'^\\d{4}', 'current', regex=True) \n",
    "        df_ori = df_ori.combine_first(df)\n",
    "        \n",
    "    \n",
    "    ## beautify some of the curren (e.g., current01_?? --> current_01_??)\n",
    "    ## not working 11/09/2023\n",
    "#     df_ori.columns = df_ori.columns.str.replace(r'current\\d+', 'current_', regex=True)\n",
    "#     df.filter(regex=r'current\\d+').columns.str.replace('current','current_')\n",
    "#     df.columns.str.contains('current\\d+',regex=True)\n",
    "\n",
    "    return df_ori            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d22caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to process the column name to obtain the current year variables \n",
    "## This function is a replicate of the previous, however, it is used for annual variables to reduce computation \n",
    "def process_current_year(df_ori):\n",
    "    ### Set the index to system:index\n",
    "    df_ori.set_index('system:index', inplace=True)\n",
    "\n",
    "    ### standardized naming - add suffix annual to all landcover\n",
    "    df = df_ori.filter(regex=r'\\d{4}').add_suffix('_annual')                     #filter all landcover and add suffix \n",
    "    df_ori = df_ori[df_ori.columns.drop(list(df_ori.filter(regex=r'\\d{4}')))]    #drop all landcover previously have the year\n",
    "    df_ori = df_ori.combine_first(df)                                            #combine back with the one added with suffix \n",
    "\n",
    "    print(\"obtaining the current year average for each of the fire point \")\n",
    "    for year in df_ori['year'].unique():\n",
    "#         print(year)\n",
    "        df = df_ori.copy()\n",
    "        regex_pattern = r'^' + str(year)     \n",
    "        df = df.query('year == @year').filter(regex=regex_pattern)\n",
    "        df.columns = df.columns.str.replace(r'^\\d{4}', 'current', regex=True) \n",
    "        df_ori = df_ori.combine_first(df)\n",
    "        \n",
    "    return df_ori            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0cd35",
   "metadata": {},
   "source": [
    "## Monthly Average in Google Earth Engine\n",
    "- obtain the monthly average from google earth engine\n",
    "- GEE doesn't allow you to export more than 5000 bands \n",
    "- Reason to extract all the monthly data - easier for data analysis work \n",
    "- Format for KBDI 202201_KBDI \n",
    "- Format for Landsat temperature 2022_01_LST (need to moreve the _ from system index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bba8ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_Avg (collection, years, months):\n",
    "    avg = []\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            #Get the system index from the first collection of every month, get only the first 6 character \n",
    "            print (year, month)\n",
    "#             system_index   = collection.filter(ee.Filter.calendarRange(year, year, 'year')) \\\n",
    "#                                   .filter(ee.Filter.calendarRange(month, month, 'month')) \\\n",
    "#                                   .first().getInfo().get('properties').get('system:index')[:6].replace('_','')\n",
    "            # instead of using the system index directly, we map the year + month (in two digit) to generalize\n",
    "            system_index = (str(year) + \"{:02d}\".format(month))\n",
    "            \n",
    "            #Get the monthly average \n",
    "            Monthly_avg = collection.filter(ee.Filter.calendarRange(year, year, 'year')) \\\n",
    "                                  .filter(ee.Filter.calendarRange(month, month, 'month')) \\\n",
    "                                  .mean() \\\n",
    "                                  .set({'month': month, 'year': year, 'system:index': system_index})\n",
    "            avg.append (Monthly_avg)\n",
    "    return ee.ImageCollection.fromImages(avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b2d6e",
   "metadata": {},
   "source": [
    "## Terra Climate\n",
    "- monthly data\n",
    "- 1958-01-01 to 2022-12-01\n",
    "- 4km (native scale) \n",
    "- seasonal / annual - in pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a77e57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the dataset and clip to the selected area \n",
    "terraclimate = ee.ImageCollection('IDAHO_EPSCOR/TERRACLIMATE') \\\n",
    "                  .filter(ee.Filter.date(fire_start, fire_end))\n",
    "terraclimate = terraclimate.map(lambda image:image.clip(feature_selected_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c72b151a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Applying Scale Factors\n",
    "### .map cannot use client side loops, this is currently the best way to apply all \n",
    "def apply_scale(image):\n",
    "    aet_s = image.select(['aet']).multiply(0.1)\n",
    "    def_s = image.select(['def']).multiply(0.1)\n",
    "    pdsi_s = image.select(['pdsi']).multiply(0.01)\n",
    "    pet_s = image.select(['pet']).multiply(0.1)\n",
    "#     pr_s = image.select(['pr']) # scale=1.0\n",
    "#     ro_s = image.select(['ro']) # scale=1.0\n",
    "    soil_s = image.select(['soil']).multiply(0.1)\n",
    "    srad_s = image.select(['srad']).multiply(0.1)\n",
    "#     swe_s = image.select(['swe'])  # scale=1.0\n",
    "    tmmn_s = image.select(['tmmn']).multiply(0.1)\n",
    "    tmmx_s = image.select(['tmmx']).multiply(0.1)\n",
    "    vap_s = image.select(['vap']).multiply(0.001)\n",
    "    vpd_s = image.select(['vpd']).multiply(0.01)\n",
    "    vs_s = image.select(['vs']).multiply(0.01)\n",
    "    return image.addBands([aet_s, def_s, pdsi_s, pet_s, soil_s, srad_s, tmmn_s, tmmx_s, vap_s, vpd_s, vs_s], overwrite=True)\n",
    "\n",
    "terraclimate = terraclimate.map(apply_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### if not too many points / date range, can directly use this (extracted one by one due to computation resource error gee)\n",
    "# ### toBands -convert a collection multi-band image containing all of the bands of every image in the collection \n",
    "# ### image collection cannot use extract_values_to_points\n",
    "# terraclimate = terraclimate.toBands()\n",
    "# work_dir = os.path.expanduser('')\n",
    "# out_csv = os.path.join(work_dir, 'terraclimate.csv')\n",
    "# geemap.extract_values_to_points(fire_dataset_points, terraclimate, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3e10d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/775c180356715d054b87f9daa584a12c-6e8447586cb4ca0cebd7afd4fdbba586:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\aet.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/b28229d86bdd5e55f372a407d38b53d7-b5c1b029f9eb55365ed599969a69f8ac:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\def.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/b67da8ae20ebc461d554c33a452a605d-780d0648e0766e8d9b328a8afd0f0fdf:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\pdsi.csv\n"
     ]
    }
   ],
   "source": [
    "### Extract variable one by one due to memory exceeded / resource exceeded \n",
    "### not recommended to use looping as the loop might break due to reliance on GEE \n",
    "selected = terraclimate.select('aet')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'aet.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('def')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'def.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('pdsi')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'pdsi.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6807dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/751c298ace8f2270109e247deae80480-9976ee9a867fb9f71dc46ae7c057a07d:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\pet.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/0241c9a958848c56fef88c296efafa81-8ba6b8ccd088b717a18a85f2036a6438:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\pr.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/452693e7febcc5ceeb1bafa5b2a768af-99aaf387dc8e80838b5825f4ee5fcb7d:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\ro.csv\n"
     ]
    }
   ],
   "source": [
    "selected = terraclimate.select('pet')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'pet.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('pr')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'pr.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('ro')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'ro.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "243bdb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/b26666c6da51ecbf3ccf33804fe4106d-f66f8a900a9406ee1d17a59343bdc1c2:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\soil.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/949bbde0b7ce4314b519fa8eb4f0c0f5-f947246058e6de7520d1031687ba0fce:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\srad.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/037756a14017f6838e3edfdb9f2f4d8f-d7e5f0ba259ff011f37059df081e6fdb:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\swe.csv\n"
     ]
    }
   ],
   "source": [
    "selected = terraclimate.select('soil')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'soil.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('srad')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'srad.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('swe')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'swe.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4907a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/b0958154f4fa62eb46a4dad56c15abfb-ac928533063106493778221bd9fc18da:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\tmmn.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/bcf15b69e30a75ee7f4f56d9627783ef-56be1c985ee5685ad9bb5358bc75f9b6:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\tmmx.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/ed7a1dfed355c99cf6ebaf65905ea586-fdcc5a6cbfe79629c2b4192dcdc39e58:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\vap.csv\n"
     ]
    }
   ],
   "source": [
    "selected = terraclimate.select('tmmn')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'tmmn.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('tmmx')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'tmmx.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('vap')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'vap.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56334907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/9abd912fe750c00952433715315189a0-5ce66ff8213ea5c1ceb3d3410e21167d:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\vpd.csv\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/1b6d195d055b3ed0bedfe00143d79e9b-c5ccc6d3b0f242eeb408378933b44841:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\vs.csv\n"
     ]
    }
   ],
   "source": [
    "selected = terraclimate.select('vpd')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'vpd.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)\n",
    "\n",
    "selected = terraclimate.select('vs')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'vs.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a3358c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Combine all the variables .csv together \n",
    "aet_df = pd.read_csv('aet.csv')\n",
    "def_df = pd.read_csv('def.csv')\n",
    "pdsi_df = pd.read_csv('pdsi.csv')\n",
    "pet_df = pd.read_csv('pet.csv')\n",
    "pr_df = pd.read_csv('pr.csv')\n",
    "ro_df = pd.read_csv('ro.csv')\n",
    "soil_df = pd.read_csv('soil.csv')\n",
    "srad_df = pd.read_csv('srad.csv')\n",
    "swe_df = pd.read_csv('swe.csv')\n",
    "tmmn_df = pd.read_csv('tmmn.csv')\n",
    "tmmx_df = pd.read_csv('tmmx.csv')\n",
    "vap_df = pd.read_csv('vap.csv')\n",
    "vpd_df = pd.read_csv('vpd.csv')\n",
    "vs_df = pd.read_csv('vs.csv')\n",
    "\n",
    "\n",
    "### Initialize the merged_df DataFrame\n",
    "merged_df = aet_df\n",
    "\n",
    "# Join each DataFrame with merged_df and  then duplicate column name will have suffix of DROP, filter away any column that contain DROP \n",
    "merged_df = merged_df.join(def_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(pdsi_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(pet_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(pr_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(ro_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(soil_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(srad_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(swe_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(tmmn_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(tmmx_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(vap_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(vpd_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(vs_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "merged_df.set_index('system:index', inplace=True)\n",
    "merged_df.to_csv('terraclimate.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f87e6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('terraclimate.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0dd5af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aet', 'def', 'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn',\n",
      "       'tmmx', 'vap', 'vpd', 'vs'],\n",
      "      dtype='object')\n",
      "calculating annual average and seasonal average for aet\n",
      "calculating annual average and seasonal average for def\n",
      "calculating annual average and seasonal average for pdsi\n",
      "calculating annual average and seasonal average for pet\n",
      "calculating annual average and seasonal average for pr\n",
      "calculating annual average and seasonal average for ro\n",
      "calculating annual average and seasonal average for soil\n",
      "calculating annual average and seasonal average for srad\n",
      "calculating annual average and seasonal average for swe\n",
      "calculating annual average and seasonal average for tmmn\n",
      "calculating annual average and seasonal average for tmmx\n",
      "calculating annual average and seasonal average for vap\n",
      "calculating annual average and seasonal average for vpd\n",
      "calculating annual average and seasonal average for vs\n",
      "obtaining the current year average for each of the fire point \n"
     ]
    }
   ],
   "source": [
    "### Load .csv - obtain annual/sesaonl average & current year values \n",
    "df = geemap.csv_to_df('terraclimate.csv')\n",
    "df = process_annual_seasonal(df)\n",
    "df.to_csv(\"terraclimate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7f266",
   "metadata": {},
   "source": [
    "## KBDI\n",
    "- 2007-01-01T00:00:00Zâ2023-07-26T00:00:00\n",
    "- daily\n",
    "- 4000 meters \n",
    "- consider mean monthly, annual, seasonal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d7d2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "KBDI = ee.ImageCollection('UTOKYO/WTLAB/KBDI/v1') \\\n",
    "  .select('KBDI') \\\n",
    "  .filterDate(fire_start, fire_end)\n",
    "KBDI = KBDI.map(lambda image:image.clip(feature_selected_states))\n",
    "bandViz = {\n",
    "  'min': 0,\n",
    "  'max': 800,\n",
    "  'palette': [\n",
    "    '001a4d', '003cb3', '80aaff', '336600', 'cccc00', 'cc9900', 'cc6600',\n",
    "    '660033'\n",
    "  ]\n",
    "}\n",
    "Map.addLayer(KBDI.mean(), bandViz, 'Keetch-Byram Drought Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "387722af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 1\n",
      "2001 2\n",
      "2001 3\n",
      "2001 4\n",
      "2001 5\n",
      "2001 6\n",
      "2001 7\n",
      "2001 8\n",
      "2001 9\n",
      "2001 10\n",
      "2001 11\n",
      "2001 12\n",
      "2002 1\n",
      "2002 2\n",
      "2002 3\n",
      "2002 4\n",
      "2002 5\n",
      "2002 6\n",
      "2002 7\n",
      "2002 8\n",
      "2002 9\n",
      "2002 10\n",
      "2002 11\n",
      "2002 12\n",
      "2003 1\n",
      "2003 2\n",
      "2003 3\n",
      "2003 4\n",
      "2003 5\n",
      "2003 6\n",
      "2003 7\n",
      "2003 8\n",
      "2003 9\n",
      "2003 10\n",
      "2003 11\n",
      "2003 12\n",
      "2004 1\n",
      "2004 2\n",
      "2004 3\n",
      "2004 4\n",
      "2004 5\n",
      "2004 6\n",
      "2004 7\n",
      "2004 8\n",
      "2004 9\n",
      "2004 10\n",
      "2004 11\n",
      "2004 12\n",
      "2005 1\n",
      "2005 2\n",
      "2005 3\n",
      "2005 4\n",
      "2005 5\n",
      "2005 6\n",
      "2005 7\n",
      "2005 8\n",
      "2005 9\n",
      "2005 10\n",
      "2005 11\n",
      "2005 12\n",
      "2006 1\n",
      "2006 2\n",
      "2006 3\n",
      "2006 4\n",
      "2006 5\n",
      "2006 6\n",
      "2006 7\n",
      "2006 8\n",
      "2006 9\n",
      "2006 10\n",
      "2006 11\n",
      "2006 12\n",
      "2007 1\n",
      "2007 2\n",
      "2007 3\n",
      "2007 4\n",
      "2007 5\n",
      "2007 6\n",
      "2007 7\n",
      "2007 8\n",
      "2007 9\n",
      "2007 10\n",
      "2007 11\n",
      "2007 12\n",
      "2008 1\n",
      "2008 2\n",
      "2008 3\n",
      "2008 4\n",
      "2008 5\n",
      "2008 6\n",
      "2008 7\n",
      "2008 8\n",
      "2008 9\n",
      "2008 10\n",
      "2008 11\n",
      "2008 12\n",
      "2009 1\n",
      "2009 2\n",
      "2009 3\n",
      "2009 4\n",
      "2009 5\n",
      "2009 6\n",
      "2009 7\n",
      "2009 8\n",
      "2009 9\n",
      "2009 10\n",
      "2009 11\n",
      "2009 12\n",
      "2010 1\n",
      "2010 2\n",
      "2010 3\n",
      "2010 4\n",
      "2010 5\n",
      "2010 6\n",
      "2010 7\n",
      "2010 8\n",
      "2010 9\n",
      "2010 10\n",
      "2010 11\n",
      "2010 12\n",
      "2011 1\n",
      "2011 2\n",
      "2011 3\n",
      "2011 4\n",
      "2011 5\n",
      "2011 6\n",
      "2011 7\n",
      "2011 8\n",
      "2011 9\n",
      "2011 10\n",
      "2011 11\n",
      "2011 12\n",
      "2012 1\n",
      "2012 2\n",
      "2012 3\n",
      "2012 4\n",
      "2012 5\n",
      "2012 6\n",
      "2012 7\n",
      "2012 8\n",
      "2012 9\n",
      "2012 10\n",
      "2012 11\n",
      "2012 12\n",
      "2013 1\n",
      "2013 2\n",
      "2013 3\n",
      "2013 4\n",
      "2013 5\n",
      "2013 6\n",
      "2013 7\n",
      "2013 8\n",
      "2013 9\n",
      "2013 10\n",
      "2013 11\n",
      "2013 12\n",
      "2014 1\n",
      "2014 2\n",
      "2014 3\n",
      "2014 4\n",
      "2014 5\n",
      "2014 6\n",
      "2014 7\n",
      "2014 8\n",
      "2014 9\n",
      "2014 10\n",
      "2014 11\n",
      "2014 12\n",
      "2015 1\n",
      "2015 2\n",
      "2015 3\n",
      "2015 4\n",
      "2015 5\n",
      "2015 6\n",
      "2015 7\n",
      "2015 8\n",
      "2015 9\n",
      "2015 10\n",
      "2015 11\n",
      "2015 12\n",
      "2016 1\n",
      "2016 2\n",
      "2016 3\n",
      "2016 4\n",
      "2016 5\n",
      "2016 6\n",
      "2016 7\n",
      "2016 8\n",
      "2016 9\n",
      "2016 10\n",
      "2016 11\n",
      "2016 12\n",
      "2017 1\n",
      "2017 2\n",
      "2017 3\n",
      "2017 4\n",
      "2017 5\n",
      "2017 6\n",
      "2017 7\n",
      "2017 8\n",
      "2017 9\n",
      "2017 10\n",
      "2017 11\n",
      "2017 12\n",
      "2018 1\n",
      "2018 2\n",
      "2018 3\n",
      "2018 4\n",
      "2018 5\n",
      "2018 6\n",
      "2018 7\n",
      "2018 8\n",
      "2018 9\n",
      "2018 10\n",
      "2018 11\n",
      "2018 12\n",
      "2019 1\n",
      "2019 2\n",
      "2019 3\n",
      "2019 4\n",
      "2019 5\n",
      "2019 6\n",
      "2019 7\n",
      "2019 8\n",
      "2019 9\n",
      "2019 10\n",
      "2019 11\n",
      "2019 12\n",
      "2020 1\n",
      "2020 2\n",
      "2020 3\n",
      "2020 4\n",
      "2020 5\n",
      "2020 6\n",
      "2020 7\n",
      "2020 8\n",
      "2020 9\n",
      "2020 10\n",
      "2020 11\n",
      "2020 12\n",
      "2021 1\n",
      "2021 2\n",
      "2021 3\n",
      "2021 4\n",
      "2021 5\n",
      "2021 6\n",
      "2021 7\n",
      "2021 8\n",
      "2021 9\n",
      "2021 10\n",
      "2021 11\n",
      "2021 12\n",
      "2022 1\n",
      "2022 2\n",
      "2022 3\n",
      "2022 4\n",
      "2022 5\n",
      "2022 6\n",
      "2022 7\n",
      "2022 8\n",
      "2022 9\n",
      "2022 10\n",
      "2022 11\n",
      "2022 12\n",
      "2023 1\n",
      "2023 2\n",
      "2023 3\n",
      "2023 4\n",
      "2023 5\n",
      "2023 6\n",
      "2023 7\n",
      "2023 8\n",
      "2023 9\n",
      "2023 10\n",
      "2023 11\n",
      "2023 12\n"
     ]
    }
   ],
   "source": [
    "# Define the years and months range for extracting annual average and monthly average \n",
    "years = range(fire_start.get('year').getInfo(), fire_end.get('year').getInfo())\n",
    "# Manually change the month if error, or it is not starting from january \n",
    "months = range(1,13)\n",
    "KBDI = monthly_Avg(KBDI,years,months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c30732aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/72cffc9ad12712c54949645e04ea2758-fccfe008ce8dc958924a6a61a898c76a:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\KBDI.csv\n"
     ]
    }
   ],
   "source": [
    "### toBands -convert a collection multi-band image containing all of the bands of every image in the collection \n",
    "### image collection cannot use extract_values_to_points\n",
    "KBDI = KBDI.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'KBDI.csv')\n",
    "### Scale Parameters are required else will come out error due to the monthly mean and etc. \n",
    "###  EEException: Image.reduceRegions: The default WGS84 projection is invalid for aggregations. Specify a scale or crs & crs_transform.\n",
    "geemap.extract_values_to_points(fire_dataset_points, KBDI, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c6b9e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['KBDI'], dtype='object')\n",
      "calculating annual average and seasonal average for KBDI\n",
      "obtaining the current year average for each of the fire point \n"
     ]
    }
   ],
   "source": [
    "### Annual/ Seaonal/ Monthly Average of the current year of fire  \n",
    "df = geemap.csv_to_df('KBDI.csv')\n",
    "# set index to system index \n",
    "df = process_annual_seasonal(df)\n",
    "df.to_csv(\"KBDI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa95db4",
   "metadata": {},
   "source": [
    "## MOD11A2.061 Terra Land Surface Temperature and Emissivity 8-Day Global 1km\n",
    "- Dataset Availability\n",
    "- 2000-02-18T00:00:00Zâ2023-07-04T00:00:00\n",
    "- 1000 meters\n",
    "- every 8 days\n",
    "- consider mean monthly, annual, seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6abaa037",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsurfacetemperature = ee.ImageCollection('MODIS/061/MOD11A2') \\\n",
    "                  .filter(ee.Filter.date(fire_start, fire_end))\n",
    "landsurfacetemperature = landsurfacetemperature.map(lambda image:image.clip(feature_selected_states))\n",
    "landsurfacetemperature = landsurfacetemperature.select('LST_Day_1km')\n",
    "landsurfacetemperatureVis = {\n",
    "  'min': 14000.0,\n",
    "  'max': 16000.0,\n",
    "  'palette': [\n",
    "    '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',\n",
    "    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',\n",
    "    '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',\n",
    "    'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',\n",
    "    'ff0000', 'de0101', 'c21301', 'a71001', '911003'\n",
    "  ],\n",
    "}\n",
    "Map.addLayer(\n",
    "    landsurfacetemperature, landsurfacetemperatureVis,\n",
    "    'Land Surface Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd98b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying Scale Factors\n",
    "### .map cannot use client side loops, this is currently the best way to apply all \n",
    "def apply_scale(image):\n",
    "    lst_s = image.select(['LST_Day_1km']).multiply(0.02)\n",
    "    return image.addBands([lst_s], overwrite=True)\n",
    "\n",
    "landsurfacetemperature = landsurfacetemperature.map(apply_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9800edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 1\n",
      "2001 2\n",
      "2001 3\n",
      "2001 4\n",
      "2001 5\n",
      "2001 6\n",
      "2001 7\n",
      "2001 8\n",
      "2001 9\n",
      "2001 10\n",
      "2001 11\n",
      "2001 12\n",
      "2002 1\n",
      "2002 2\n",
      "2002 3\n",
      "2002 4\n",
      "2002 5\n",
      "2002 6\n",
      "2002 7\n",
      "2002 8\n",
      "2002 9\n",
      "2002 10\n",
      "2002 11\n",
      "2002 12\n",
      "2003 1\n",
      "2003 2\n",
      "2003 3\n",
      "2003 4\n",
      "2003 5\n",
      "2003 6\n",
      "2003 7\n",
      "2003 8\n",
      "2003 9\n",
      "2003 10\n",
      "2003 11\n",
      "2003 12\n",
      "2004 1\n",
      "2004 2\n",
      "2004 3\n",
      "2004 4\n",
      "2004 5\n",
      "2004 6\n",
      "2004 7\n",
      "2004 8\n",
      "2004 9\n",
      "2004 10\n",
      "2004 11\n",
      "2004 12\n",
      "2005 1\n",
      "2005 2\n",
      "2005 3\n",
      "2005 4\n",
      "2005 5\n",
      "2005 6\n",
      "2005 7\n",
      "2005 8\n",
      "2005 9\n",
      "2005 10\n",
      "2005 11\n",
      "2005 12\n",
      "2006 1\n",
      "2006 2\n",
      "2006 3\n",
      "2006 4\n",
      "2006 5\n",
      "2006 6\n",
      "2006 7\n",
      "2006 8\n",
      "2006 9\n",
      "2006 10\n",
      "2006 11\n",
      "2006 12\n",
      "2007 1\n",
      "2007 2\n",
      "2007 3\n",
      "2007 4\n",
      "2007 5\n",
      "2007 6\n",
      "2007 7\n",
      "2007 8\n",
      "2007 9\n",
      "2007 10\n",
      "2007 11\n",
      "2007 12\n",
      "2008 1\n",
      "2008 2\n",
      "2008 3\n",
      "2008 4\n",
      "2008 5\n",
      "2008 6\n",
      "2008 7\n",
      "2008 8\n",
      "2008 9\n",
      "2008 10\n",
      "2008 11\n",
      "2008 12\n",
      "2009 1\n",
      "2009 2\n",
      "2009 3\n",
      "2009 4\n",
      "2009 5\n",
      "2009 6\n",
      "2009 7\n",
      "2009 8\n",
      "2009 9\n",
      "2009 10\n",
      "2009 11\n",
      "2009 12\n",
      "2010 1\n",
      "2010 2\n",
      "2010 3\n",
      "2010 4\n",
      "2010 5\n",
      "2010 6\n",
      "2010 7\n",
      "2010 8\n",
      "2010 9\n",
      "2010 10\n",
      "2010 11\n",
      "2010 12\n",
      "2011 1\n",
      "2011 2\n",
      "2011 3\n",
      "2011 4\n",
      "2011 5\n",
      "2011 6\n",
      "2011 7\n",
      "2011 8\n",
      "2011 9\n",
      "2011 10\n",
      "2011 11\n",
      "2011 12\n",
      "2012 1\n",
      "2012 2\n",
      "2012 3\n",
      "2012 4\n",
      "2012 5\n",
      "2012 6\n",
      "2012 7\n",
      "2012 8\n",
      "2012 9\n",
      "2012 10\n",
      "2012 11\n",
      "2012 12\n",
      "2013 1\n",
      "2013 2\n",
      "2013 3\n",
      "2013 4\n",
      "2013 5\n",
      "2013 6\n",
      "2013 7\n",
      "2013 8\n",
      "2013 9\n",
      "2013 10\n",
      "2013 11\n",
      "2013 12\n",
      "2014 1\n",
      "2014 2\n",
      "2014 3\n",
      "2014 4\n",
      "2014 5\n",
      "2014 6\n",
      "2014 7\n",
      "2014 8\n",
      "2014 9\n",
      "2014 10\n",
      "2014 11\n",
      "2014 12\n",
      "2015 1\n",
      "2015 2\n",
      "2015 3\n",
      "2015 4\n",
      "2015 5\n",
      "2015 6\n",
      "2015 7\n",
      "2015 8\n",
      "2015 9\n",
      "2015 10\n",
      "2015 11\n",
      "2015 12\n",
      "2016 1\n",
      "2016 2\n",
      "2016 3\n",
      "2016 4\n",
      "2016 5\n",
      "2016 6\n",
      "2016 7\n",
      "2016 8\n",
      "2016 9\n",
      "2016 10\n",
      "2016 11\n",
      "2016 12\n",
      "2017 1\n",
      "2017 2\n",
      "2017 3\n",
      "2017 4\n",
      "2017 5\n",
      "2017 6\n",
      "2017 7\n",
      "2017 8\n",
      "2017 9\n",
      "2017 10\n",
      "2017 11\n",
      "2017 12\n",
      "2018 1\n",
      "2018 2\n",
      "2018 3\n",
      "2018 4\n",
      "2018 5\n",
      "2018 6\n",
      "2018 7\n",
      "2018 8\n",
      "2018 9\n",
      "2018 10\n",
      "2018 11\n",
      "2018 12\n",
      "2019 1\n",
      "2019 2\n",
      "2019 3\n",
      "2019 4\n",
      "2019 5\n",
      "2019 6\n",
      "2019 7\n",
      "2019 8\n",
      "2019 9\n",
      "2019 10\n",
      "2019 11\n",
      "2019 12\n",
      "2020 1\n",
      "2020 2\n",
      "2020 3\n",
      "2020 4\n",
      "2020 5\n",
      "2020 6\n",
      "2020 7\n",
      "2020 8\n",
      "2020 9\n",
      "2020 10\n",
      "2020 11\n",
      "2020 12\n",
      "2021 1\n",
      "2021 2\n",
      "2021 3\n",
      "2021 4\n",
      "2021 5\n",
      "2021 6\n",
      "2021 7\n",
      "2021 8\n",
      "2021 9\n",
      "2021 10\n",
      "2021 11\n",
      "2021 12\n",
      "2022 1\n",
      "2022 2\n",
      "2022 3\n",
      "2022 4\n",
      "2022 5\n",
      "2022 6\n",
      "2022 7\n",
      "2022 8\n",
      "2022 9\n",
      "2022 10\n",
      "2022 11\n",
      "2022 12\n",
      "2023 1\n",
      "2023 2\n",
      "2023 3\n",
      "2023 4\n",
      "2023 5\n",
      "2023 6\n",
      "2023 7\n",
      "2023 8\n",
      "2023 9\n",
      "2023 10\n",
      "2023 11\n",
      "2023 12\n"
     ]
    }
   ],
   "source": [
    "# Define the years and months range for extracting annual average and monthly average \n",
    "years = range(fire_start.get('year').getInfo(), fire_end.get('year').getInfo())\n",
    "\n",
    "# Manually change the month if error, or it is not starting from january \n",
    "months = range(1,13)\n",
    "\n",
    "### Obtain the monthly average values\n",
    "landsurfacetemperature = monthly_Avg(landsurfacetemperature,years,months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59946f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/8a035f0abe9bc71531a94e3a83fce7d9-b405a830d2028056669b756240c02755:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\landsurfacetemperature.csv\n"
     ]
    }
   ],
   "source": [
    "### toBands -convert a collection multi-band image containing all of the bands of every image in the collection \n",
    "### image collection cannot use extract_values_to_points\n",
    "landsurfacetemperature = landsurfacetemperature.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'landsurfacetemperature.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, landsurfacetemperature, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa36e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LST'], dtype='object')\n",
      "calculating annual average and seasonal average for LST\n",
      "obtaining the current year average for each of the fire point \n"
     ]
    }
   ],
   "source": [
    "### Annual/ Seaonal/ Monthly Average of the current year of fire  \n",
    "df = geemap.csv_to_df('landsurfacetemperature.csv')\n",
    "# set index to system index \n",
    "df = process_annual_seasonal(df)\n",
    "df.to_csv(\"landsurfacetemperature.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a11aa",
   "metadata": {},
   "source": [
    "## NDVI MOD13Q1.061 Terra \n",
    "- Dataset Availability\n",
    "2000-02-18T00:00:00Zâ2023-06-26T00:00:00\n",
    "- 16 days \n",
    "- 250m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce11578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_evi = ee.ImageCollection('MODIS/061/MOD13Q1') \\\n",
    "                  .filter(ee.Filter.date(fire_start, fire_end))\n",
    "ndvi_evi = ndvi_evi.map(lambda image:image.clip(feature_selected_states))\n",
    "ndvi_evi = ndvi_evi.select('NDVI','EVI')\n",
    "\n",
    "# # Visualize\n",
    "# ndvi = dataset.select('NDVI')\n",
    "# ndviVis = {\n",
    "#   'min': 0.0,\n",
    "#   'max': 8000.0,\n",
    "#   'palette': [\n",
    "#     'FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n",
    "#     '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n",
    "#     '012E01', '011D01', '011301'\n",
    "#   ],\n",
    "# }\n",
    "# Map.addLayer(ndvi, ndviVis, 'NDVI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5729de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying Scale Factors\n",
    "### .map cannot use client side loops, this is currently the best way to apply all \n",
    "def apply_scale(image):\n",
    "    ndvi_s = image.select(['NDVI']).multiply(0.0001)\n",
    "    evi_s = image.select(['EVI']).multiply(0.0001)\n",
    "    return image.addBands([ndvi_s,evi_s], overwrite=True)\n",
    "\n",
    "ndvi_evi = ndvi_evi.map(apply_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e82d23c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 1\n",
      "2001 2\n",
      "2001 3\n",
      "2001 4\n",
      "2001 5\n",
      "2001 6\n",
      "2001 7\n",
      "2001 8\n",
      "2001 9\n",
      "2001 10\n",
      "2001 11\n",
      "2001 12\n",
      "2002 1\n",
      "2002 2\n",
      "2002 3\n",
      "2002 4\n",
      "2002 5\n",
      "2002 6\n",
      "2002 7\n",
      "2002 8\n",
      "2002 9\n",
      "2002 10\n",
      "2002 11\n",
      "2002 12\n",
      "2003 1\n",
      "2003 2\n",
      "2003 3\n",
      "2003 4\n",
      "2003 5\n",
      "2003 6\n",
      "2003 7\n",
      "2003 8\n",
      "2003 9\n",
      "2003 10\n",
      "2003 11\n",
      "2003 12\n",
      "2004 1\n",
      "2004 2\n",
      "2004 3\n",
      "2004 4\n",
      "2004 5\n",
      "2004 6\n",
      "2004 7\n",
      "2004 8\n",
      "2004 9\n",
      "2004 10\n",
      "2004 11\n",
      "2004 12\n",
      "2005 1\n",
      "2005 2\n",
      "2005 3\n",
      "2005 4\n",
      "2005 5\n",
      "2005 6\n",
      "2005 7\n",
      "2005 8\n",
      "2005 9\n",
      "2005 10\n",
      "2005 11\n",
      "2005 12\n",
      "2006 1\n",
      "2006 2\n",
      "2006 3\n",
      "2006 4\n",
      "2006 5\n",
      "2006 6\n",
      "2006 7\n",
      "2006 8\n",
      "2006 9\n",
      "2006 10\n",
      "2006 11\n",
      "2006 12\n",
      "2007 1\n",
      "2007 2\n",
      "2007 3\n",
      "2007 4\n",
      "2007 5\n",
      "2007 6\n",
      "2007 7\n",
      "2007 8\n",
      "2007 9\n",
      "2007 10\n",
      "2007 11\n",
      "2007 12\n",
      "2008 1\n",
      "2008 2\n",
      "2008 3\n",
      "2008 4\n",
      "2008 5\n",
      "2008 6\n",
      "2008 7\n",
      "2008 8\n",
      "2008 9\n",
      "2008 10\n",
      "2008 11\n",
      "2008 12\n",
      "2009 1\n",
      "2009 2\n",
      "2009 3\n",
      "2009 4\n",
      "2009 5\n",
      "2009 6\n",
      "2009 7\n",
      "2009 8\n",
      "2009 9\n",
      "2009 10\n",
      "2009 11\n",
      "2009 12\n",
      "2010 1\n",
      "2010 2\n",
      "2010 3\n",
      "2010 4\n",
      "2010 5\n",
      "2010 6\n",
      "2010 7\n",
      "2010 8\n",
      "2010 9\n",
      "2010 10\n",
      "2010 11\n",
      "2010 12\n",
      "2011 1\n",
      "2011 2\n",
      "2011 3\n",
      "2011 4\n",
      "2011 5\n",
      "2011 6\n",
      "2011 7\n",
      "2011 8\n",
      "2011 9\n",
      "2011 10\n",
      "2011 11\n",
      "2011 12\n",
      "2012 1\n",
      "2012 2\n",
      "2012 3\n",
      "2012 4\n",
      "2012 5\n",
      "2012 6\n",
      "2012 7\n",
      "2012 8\n",
      "2012 9\n",
      "2012 10\n",
      "2012 11\n",
      "2012 12\n",
      "2013 1\n",
      "2013 2\n",
      "2013 3\n",
      "2013 4\n",
      "2013 5\n",
      "2013 6\n",
      "2013 7\n",
      "2013 8\n",
      "2013 9\n",
      "2013 10\n",
      "2013 11\n",
      "2013 12\n",
      "2014 1\n",
      "2014 2\n",
      "2014 3\n",
      "2014 4\n",
      "2014 5\n",
      "2014 6\n",
      "2014 7\n",
      "2014 8\n",
      "2014 9\n",
      "2014 10\n",
      "2014 11\n",
      "2014 12\n",
      "2015 1\n",
      "2015 2\n",
      "2015 3\n",
      "2015 4\n",
      "2015 5\n",
      "2015 6\n",
      "2015 7\n",
      "2015 8\n",
      "2015 9\n",
      "2015 10\n",
      "2015 11\n",
      "2015 12\n",
      "2016 1\n",
      "2016 2\n",
      "2016 3\n",
      "2016 4\n",
      "2016 5\n",
      "2016 6\n",
      "2016 7\n",
      "2016 8\n",
      "2016 9\n",
      "2016 10\n",
      "2016 11\n",
      "2016 12\n",
      "2017 1\n",
      "2017 2\n",
      "2017 3\n",
      "2017 4\n",
      "2017 5\n",
      "2017 6\n",
      "2017 7\n",
      "2017 8\n",
      "2017 9\n",
      "2017 10\n",
      "2017 11\n",
      "2017 12\n",
      "2018 1\n",
      "2018 2\n",
      "2018 3\n",
      "2018 4\n",
      "2018 5\n",
      "2018 6\n",
      "2018 7\n",
      "2018 8\n",
      "2018 9\n",
      "2018 10\n",
      "2018 11\n",
      "2018 12\n",
      "2019 1\n",
      "2019 2\n",
      "2019 3\n",
      "2019 4\n",
      "2019 5\n",
      "2019 6\n",
      "2019 7\n",
      "2019 8\n",
      "2019 9\n",
      "2019 10\n",
      "2019 11\n",
      "2019 12\n",
      "2020 1\n",
      "2020 2\n",
      "2020 3\n",
      "2020 4\n",
      "2020 5\n",
      "2020 6\n",
      "2020 7\n",
      "2020 8\n",
      "2020 9\n",
      "2020 10\n",
      "2020 11\n",
      "2020 12\n",
      "2021 1\n",
      "2021 2\n",
      "2021 3\n",
      "2021 4\n",
      "2021 5\n",
      "2021 6\n",
      "2021 7\n",
      "2021 8\n",
      "2021 9\n",
      "2021 10\n",
      "2021 11\n",
      "2021 12\n",
      "2022 1\n",
      "2022 2\n",
      "2022 3\n",
      "2022 4\n",
      "2022 5\n",
      "2022 6\n",
      "2022 7\n",
      "2022 8\n",
      "2022 9\n",
      "2022 10\n",
      "2022 11\n",
      "2022 12\n",
      "2023 1\n",
      "2023 2\n",
      "2023 3\n",
      "2023 4\n",
      "2023 5\n",
      "2023 6\n",
      "2023 7\n",
      "2023 8\n",
      "2023 9\n",
      "2023 10\n",
      "2023 11\n",
      "2023 12\n"
     ]
    }
   ],
   "source": [
    "### Obtain the monthly average values\n",
    "ndvi_evi = monthly_Avg(ndvi_evi,years,months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebac48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Cannot extract all of them together - memory exceeded and resource exceeded \n",
    "# ### toBands -convert a collection multi-band image containing all of the bands of every image in the collection \n",
    "# ### image collection cannot use extract_values_to_points\n",
    "# ndvi_evi = ndvi_evi.toBands()\n",
    "# work_dir = os.path.expanduser('')\n",
    "# out_csv = os.path.join(work_dir, 'ndvi_evi.csv')\n",
    "# geemap.extract_values_to_points(fire_dataset_points, ndvi_evi, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eec2d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/00a981e62d913b597b8c24a7cdb6b89f-6a3376800744380b2bfdb17886839026:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\ndvi.csv\n"
     ]
    }
   ],
   "source": [
    "### Extract variable one by one due to memory exceeded / resource exceeded \n",
    "### not recommended to use looping as the loop might break due to reliance on GEE \n",
    "selected = ndvi_evi.select('NDVI')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'ndvi.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21b1d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/7cf71928839346483f0b94733094bf34-15978db1ca08be2dee7f1d9b23ccf391:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\evi.csv\n"
     ]
    }
   ],
   "source": [
    "selected = ndvi_evi.select('EVI')\n",
    "selected = selected.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'evi.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, selected, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1bd47c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine all the variables .csv together \n",
    "ndvi_df = pd.read_csv('ndvi.csv')\n",
    "evi_df = pd.read_csv('evi.csv')\n",
    "\n",
    "## join the csv files, then duplicate column name will have suffix of DROP, filter away any column that contain DROP \n",
    "merged_df = ndvi_df.join(evi_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "merged_df.set_index('system:index', inplace=True)\n",
    "merged_df.to_csv('ndvi_evi.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "46df8856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NDVI', 'EVI'], dtype='object')\n",
      "calculating annual average and seasonal average for NDVI\n",
      "calculating annual average and seasonal average for EVI\n",
      "obtaining the current year average for each of the fire point \n"
     ]
    }
   ],
   "source": [
    "### Annual/ Seaonal/ Monthly Average of the current year of fire  \n",
    "df = geemap.csv_to_df('ndvi_evi.csv')\n",
    "# set index to system index \n",
    "df = process_annual_seasonal(df)\n",
    "df.to_csv(\"ndvi_evi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007539a0",
   "metadata": {},
   "source": [
    "## MODIS MCD12Q1 Landcover\n",
    "- 2001-2021\n",
    "- annual / yearly\n",
    "- 500m\n",
    "- task: read on the bands Land covert ype by different organization and etc. before deciding? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "531696ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCD12Q1 = ee.ImageCollection('MODIS/061/MCD12Q1') \\\n",
    "          .filterDate(fire_start, fire_end)\n",
    "MCD12Q1 = MCD12Q1.map(lambda image:image.clip(feature_selected_states))\n",
    "MCD12Q1 = MCD12Q1.select('LC_Type2')\n",
    "\n",
    "MCD12Q1Vis = {\n",
    "  'min': 0.0,\n",
    "  'max': 15.0,\n",
    "  'palette': [\n",
    "        '1c0dff', '05450a', '086a10', '54a708', '78d203', \n",
    "        '009900', 'c6b044', 'dcd159', 'dade48', 'fbff13', \n",
    "        'b6ff05', '27ff87', 'c24f44', 'a5a5a5', 'ff6d4c', 'f9ffa4'\n",
    "  ],\n",
    "}\n",
    "Map.addLayer(MCD12Q1, MCD12Q1Vis, 'MCD12Q1 UMD land cover ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f7be36d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/828d02dc4acb6a6563917b2d30942a52-52703c86411894df7d3f1f8857bdd919:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\MCD12Q1.csv\n"
     ]
    }
   ],
   "source": [
    "### toBands -convert a collection multi-band image containing all of the bands of every image in the collection \n",
    "### image collection cannot use extract_values_to_points\n",
    "MCD12Q1 = MCD12Q1.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'MCD12Q1.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, MCD12Q1, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e33e8406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining the current year average for each of the fire point \n"
     ]
    }
   ],
   "source": [
    "### Annual/ Seaonal/ Monthly Average of the current year of fire  \n",
    "df = geemap.csv_to_df('MCD12Q1.csv')\n",
    "\n",
    "\n",
    "### standardized the naming as other variables  (YYYYMM_VariableName)\n",
    "df.columns = df.columns.str.replace('_','',2)\n",
    "# df.columns = df.columns.str.replace('-','')\n",
    "\n",
    "### Annual/ Seaonal/ Monthly Average of the current year of fire \n",
    "### Although no seasonal/monthly, it will extract the current year \n",
    "# df = process_annual_seasonal(df)\n",
    "df = process_current_year(df)\n",
    "\n",
    "### Process landcover dataset \n",
    "\n",
    "# Define the mapping of values and class \n",
    "mcd12q1_class_mapping = {\n",
    "    0: 'Water Bodies',\n",
    "    1: 'Evergreen Needleleaf Forests',\n",
    "    2: 'Evergreen Broadleaf Forests',\n",
    "    3: 'Deciduous Needleleaf Forests',\n",
    "    4: 'Deciduous Broadleaf Forests',\n",
    "    5: 'Mixed Forests',\n",
    "    6: 'Closed Shrublands',\n",
    "    7: 'Open Shrublands',\n",
    "    8: 'Woody Savannas',\n",
    "    9: 'Savannas',\n",
    "    10: 'Grasslands',\n",
    "    11: 'Permanent Wetlands',\n",
    "    12: 'Croplands',\n",
    "    13: 'Urban and Built-up Lands',\n",
    "    14: 'Cropland/Natural Vegetation Mosaics',\n",
    "    15: 'Non-Vegetated Lands'\n",
    "}\n",
    "\n",
    "# Use the replace() function to populate classname\n",
    "# Merge with the original dataframe \n",
    "df_classname = df.filter(regex='.*LC_').replace(mcd12q1_class_mapping).add_suffix(\"_classname\")\n",
    "df = df.merge(df_classname, on='system:index', how='outer')\n",
    "\n",
    "df.to_csv(\"MCD12Q1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a6427389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('MCD12Q1.csv')\n",
    "### For non-fire points, use the latest year available\n",
    "df.loc[df['fire'] == 0, 'current0101_LC_Type2_annual'] = df['20220101_LC_Type2_annual']\n",
    "df.loc[df['fire'] == 0, 'current0101_LC_Type2_annual_classname'] = df['20220101_LC_Type2_annual_classname']\n",
    "df.to_csv(\"MCD12Q1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1515147",
   "metadata": {},
   "source": [
    "## Human Footprint\n",
    "- https://wcshumanfootprint.org/data-access\n",
    "- 300m\n",
    "- 2001 â 2020 \n",
    "- The footprint is a simple weighted sum of maps of where people live (population density), where we build infrastructure (including roads, railways, factories, and other kinds of infrastructure), where we can go (accessibility), and where we use electrical energy, a proxy for access to industrial energy supplies, as measured by the night-time lights.\n",
    "- Just take the human foot print\n",
    "- Contain other attribute as well : \n",
    "- Infrastructure, Landuse\tPopulation density, Power,\tRailway, Roads,\tWater\n",
    "- Consider in the future for anthropogenic analysis (calculating distance from the fire location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fbe4a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_impact_index = ee.ImageCollection(\"projects/HII/v1/hii\") \\\n",
    "                        .filter(ee.Filter.date(fire_start, fire_end))\n",
    "human_impact_index = human_impact_index.map(lambda image:image.clip(feature_selected_states))\n",
    "# hiiviz = {'min': 5, 'max': 5000, 'palette': [\"224f1a\",\"a3ff76\",\"feff6f\",\"a09568\",\"ffa802\",\"f7797c\",\"fb0102\",\"d87136\",\"a90086\",\"7a1ca5\",\"421137\",\"000000\"]}\n",
    "# Map.addLayer(human_impact_index, hiiviz, \"Human Impact Index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1f92938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/022fa4a42cbc61c8ebc41a5679f43488-2d822d5022ab2389e8fbc454c091d3d8:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\hii.csv\n"
     ]
    }
   ],
   "source": [
    "### toBands -convert a collection multi-band image containing all of the bands of every image in the collection \n",
    "### image collection cannot use extract_values_to_points\n",
    "human_impact_index = human_impact_index.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'hii.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, human_impact_index, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ef61411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining the current year average for each of the fire point \n"
     ]
    }
   ],
   "source": [
    "### Annual/ Seaonal/ Monthly Average of the current year of fire  \n",
    "df = geemap.csv_to_df('hii.csv')\n",
    "\n",
    "### standardized the naming as other variables  (YYYYMM_VariableName)\n",
    "df.columns = df.columns.str.replace('hii_','')\n",
    "df.columns = df.columns.str.replace('-','')\n",
    "\n",
    "### Although no seasonal/monthly, it will extract the current year \n",
    "df = process_current_year(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "13b76740",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For non-fire points, use the latest year available\n",
    "df.loc[df['fire'] == 0, 'current0101_hii_annual'] = df['20200101_hii_annual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c74256c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"hii.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77bd70",
   "metadata": {},
   "source": [
    "## VIIRS Nighttime Day/Night Annual Band Composites (V2.1)\n",
    "- annual\n",
    "- 2012-2021\n",
    "- Cloud free \n",
    "- 500m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09c897ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nighttime = ee.ImageCollection('NOAA/VIIRS/DNB/ANNUAL_V21') \\\n",
    "                .filter(ee.Filter.date(fire_start, fire_end))\n",
    "nighttime = nighttime.map(lambda image:image.clip(feature_selected_states))\n",
    "nighttime = nighttime.select('average')\n",
    "Map.addLayer(nighttime, {}, 'NOAA/VIIRS/DNB/ANNUAL_V21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b851f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/1bb8abdcf6a293d4c86f1fa45adcb6ec-8c6c6cc4d440a8e079af83dbee15f77b:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\nighttime1.csv\n"
     ]
    }
   ],
   "source": [
    "### toBands -convert a collection multi-band image containing all of the bands of every image in the collection \n",
    "### image collection cannot use extract_values_to_points\n",
    "nighttime = nighttime.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'nighttime1.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, nighttime, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f85615",
   "metadata": {},
   "source": [
    "### VIIRS Nighttime Day/Night Annual Band Composites (V2.2)\n",
    "- https://developers.google.com/earth-engine/datasets/catalog/NOAA_VIIRS_DNB_ANNUAL_V22\n",
    "- 2022 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c0d989d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nighttime = ee.ImageCollection('NOAA/VIIRS/DNB/ANNUAL_V22') \\\n",
    "                .filter(ee.Filter.date(fire_start, fire_end))\n",
    "nighttime = nighttime.map(lambda image:image.clip(feature_selected_states))\n",
    "nighttime = nighttime.select('average')\n",
    "Map.addLayer(nighttime, {}, 'NOAA/VIIRS/DNB/ANNUAL_V22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0d1c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/75c9ca50953fb8594e68688d27f07c80-aa566820daae75815d10dd68ec076c85:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\nighttime2.csv\n"
     ]
    }
   ],
   "source": [
    "### toBands -convert a collection multi-band image containing all of the bands of every image in the collection \n",
    "### image collection cannot use extract_values_to_points\n",
    "nighttime = nighttime.toBands()\n",
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'nighttime2.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, nighttime, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ec845ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Because the V2.2 only contain single image, the naming is different\n",
    "### To standardized, we updated the column 'first' to '20220101_average'\n",
    "\n",
    "df = geemap.csv_to_df('nighttime2.csv')\n",
    "df.rename(columns={'first': '20220101_average'}, inplace=True)\n",
    "df.set_index('system:index', inplace=True)\n",
    "df.to_csv(\"nighttime2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e7561",
   "metadata": {},
   "source": [
    "### Combine V2.1 and V2.2, extract Current Year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a532e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine the nighttime from V2.1 and V2.2\n",
    "nighttime_df1 = pd.read_csv('nighttime1.csv')\n",
    "nighttime_df2 = pd.read_csv('nighttime2.csv')\n",
    "\n",
    "### join the csv files, then duplicate column name will have suffix of DROP, filter away any column that contain DROP \n",
    "merged_df = nighttime_df1.join(nighttime_df2, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df.set_index('system:index', inplace=True)\n",
    "merged_df.to_csv(\"nighttime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e32853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtaining the current year average for each of the fire point \n"
     ]
    }
   ],
   "source": [
    "### Process dataset / rename column name\n",
    "df = geemap.csv_to_df('nighttime.csv')\n",
    "\n",
    "### Although no seasonal/monthly, it will extract the current year \n",
    "df = process_current_year(df)\n",
    "\n",
    "# rename the column \n",
    "# add suffix to the original data\n",
    "# drop the original data \n",
    "# Merge with the original dataframe \n",
    "df_classname = df.filter(regex='.*average').add_suffix(\"_nighttime\")\n",
    "df = df[df.columns.drop(list(df.filter(regex='.*average')))]\n",
    "df = df.merge(df_classname, on='system:index', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cf4c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For non-fire points, use the latest year available\n",
    "df.loc[df['fire'] == 0, 'current0101_average_annual_nighttime'] = df['20220101_average_annual_nighttime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "451a3456",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"nighttime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e5e9c",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57713b",
   "metadata": {},
   "source": [
    "## ESA landcover - Image\n",
    "- refer https://worldcover2020.esa.int/data/docs/WorldCover_PUM_V1.1.pdf for details related to the label \n",
    "- 2021-01-01 to 2022-01-01\n",
    "- Annual\t\n",
    "- Landcover (Map)\n",
    "- static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7d8062e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "esa = ee.ImageCollection('ESA/WorldCover/v200')\n",
    "esa = esa.map(lambda image:image.clip(feature_selected_states))\n",
    "esa = esa.first()\n",
    "# dataset_landcover = ee.Filter.bounds(feature_selected_states)\n",
    "visualization = {\n",
    "  'bands': ['Map'],\n",
    "}\n",
    "\n",
    "# Map.centerObject(dataset_landcover)\n",
    "# Map.addLayer(esa, visualization, \"esa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00082ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/d4aed31402147343dd991480bbcea66b-cc61fae5e8269f85ecda8334f0185ef3:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\esa.csv\n"
     ]
    }
   ],
   "source": [
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'esa.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, esa, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15fe836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process esa landcover dataset \n",
    "df = geemap.csv_to_df('esa.csv')\n",
    "df.set_index('system:index', inplace=True)\n",
    "\n",
    "# rename the column first to esa_class_values\n",
    "df = df.rename(columns={'first':'esa_class_values'})\n",
    "\n",
    "# Define the mapping of esa_class_values to esa_class_name\n",
    "esa_class_mapping = {\n",
    "    10: 'Tree cover',\n",
    "    20: 'Shrubland',\n",
    "    30: 'Grassland',\n",
    "    40: 'Cropland',\n",
    "    50: 'Built-up',\n",
    "    60: 'Bare / sparse vegetation',\n",
    "    70: 'Snow and ice',\n",
    "    80: 'Permanent water bodies',\n",
    "    90: 'Herbaceous wetland',\n",
    "    95: 'Mangroves',\n",
    "    100: 'Moss and lichen'\n",
    "}\n",
    "\n",
    "# Use the replace() function to populate esa_class_name\n",
    "df['esa_class_name'] = df['esa_class_values'].replace(esa_class_mapping)\n",
    "df.to_csv(\"esa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae94d4",
   "metadata": {},
   "source": [
    "## NASADEM\n",
    "- NASADEM  [22]\n",
    "- (static) \t\t\n",
    "- 2000-02-11 to 2000-02-22\t\n",
    "- 30m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c374d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_dem = ee.Image('NASA/NASADEM_HGT/001').select('elevation')\n",
    "dataset_dem = dataset_dem.clip(feature_selected_states)\n",
    "\n",
    "### Calculate slope. Units are degrees, range is [0,90).\n",
    "# dataset_slope = ee.Terrain.slope(dataset_dem)\n",
    "\n",
    "### Calculate aspect. Units are degrees where 0=N, 90=E, 180=S, 270=W.\n",
    "# dataset_aspect = ee.Terrain.aspect(dataset_dem)\n",
    "\n",
    "### Use the ee.Terrain.products function to calculate slope, aspect, and\n",
    "### hillshade simultaneously. The output bands are appended to the input image.\n",
    "### Hillshade is calculated based on illumination azimuth=270, elevation=45.\n",
    "dataset_terrain = ee.Terrain.products(dataset_dem)\n",
    "# print('ee.Terrain.products bands', terrain.bandNames())\n",
    "\n",
    "### Display slope and aspect and terrain layers on the map.\n",
    "# Map.addLayer(dataset_slope, {'min': 0, 'max': 89.99}, 'Slope')\n",
    "# Map.addLayer(dataset_aspect, {'min': 0, 'max': 359.99}, 'Aspect')\n",
    "Map.addLayer(dataset_terrain.select('hillshade'), {'min': 0, 'max': 255}, 'Hillshade')\n",
    "# Map.setCenter(102.206, 3.744, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de3f23c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/6ee49c393e8fd177760039d0d425eacb-5503fc9814cf41d4893e3961fe2e864e:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\dem.csv\n"
     ]
    }
   ],
   "source": [
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'dem.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, dataset_terrain, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c773b04",
   "metadata": {},
   "source": [
    "## World Settlement Footprint 2015 \n",
    "- 2015-2016\n",
    "- 10m \n",
    "- if want to use, may need to consider like (within 500m) got settlement boh? - future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc2d1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsf = ee.Image(\"DLR/WSF/WSF2015/v1\").select('settlement')\n",
    "# dataset_wsf = dataset_wsf.clip(feature_selected_states)\n",
    "dataset_wsf = dataset_wsf.unmask(0)\n",
    "\n",
    "# opacity = 0.75\n",
    "# blackBackground = ee.Image(0)\n",
    "# Map.addLayer(blackBackground, None, \"Black background\", True, opacity)\n",
    "\n",
    "# visualization = {\n",
    "#   'min': 0,\n",
    "#   'max': 255,\n",
    "# }\n",
    "# Map.addLayer(dataset_wsf, visualization, \"Human settlement areas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b3ec844",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/c011a58a6d674472adcb3835187fa619-a0ed0aa83359da2cb5e8c8427e3934f9:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to D:\\geemap_phd\\wsf.csv\n"
     ]
    }
   ],
   "source": [
    "work_dir = os.path.expanduser('')\n",
    "out_csv = os.path.join(work_dir, 'wsf.csv')\n",
    "geemap.extract_values_to_points(fire_dataset_points, dataset_wsf, out_csv, geometries=True, scale=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "24a25aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process dataset / rename column name\n",
    "df = geemap.csv_to_df('wsf.csv')\n",
    "df.set_index('system:index', inplace=True)\n",
    "\n",
    "# rename the column first to esa_class_values\n",
    "df = df.rename(columns={'first':'world_settlement_footprint'})\n",
    "df.to_csv(\"wsf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4422c",
   "metadata": {},
   "source": [
    "# Combined CSV Module\n",
    "- some codes to combine all the csv into a single csv file\n",
    "- working already, but may need some cleanup of the codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae6052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a5d44abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### list of csv names \n",
    "### combined based on the column \n",
    "### Read each CSV file into separate pandas DataFrames\n",
    "terraclimate_df = pd.read_csv('terraclimate.csv')\n",
    "ndvi_evi_df = pd.read_csv('ndvi_evi.csv')\n",
    "landsurfacetemperature_df = pd.read_csv('landsurfacetemperature.csv')\n",
    "esa_df = pd.read_csv('esa.csv')\n",
    "dem_df = pd.read_csv('dem.csv')\n",
    "KBDI_df = pd.read_csv('KBDI.csv')\n",
    "MCD12Q1_df = pd.read_csv('MCD12Q1.csv')\n",
    "wsf_df = pd.read_csv('wsf.csv')\n",
    "hii_df = pd.read_csv('hii.csv')\n",
    "nighttime_df = pd.read_csv('nighttime.csv')\n",
    "fire_dataset = pd.read_csv('fire_dataset.csv')\n",
    "\n",
    "\n",
    "## join the csv files, then duplicate column name will have suffix of DROP, filter away any column that contain DROP \n",
    "merged_df = terraclimate_df.join(ndvi_evi_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(landsurfacetemperature_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(esa_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(dem_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(KBDI_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(MCD12Q1_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(wsf_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(hii_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(nighttime_df, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "merged_df = merged_df.join(fire_dataset, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "\n",
    "### Print the combined DataFrame (optional)\n",
    "# print(merged_df)\n",
    "\n",
    "### Save the combined DataFrame to a new CSV file\n",
    "merged_df.set_index('system:index', inplace=True)\n",
    "merged_df.to_csv('combined_data.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0904b",
   "metadata": {},
   "source": [
    "## Filter for Journal - \"current-annual\"l\n",
    "- simple filter only\n",
    "- filter current-annual attributes for analysis\n",
    "- the dataset after filtered will be analyzed with Fir\n",
    "- For thesis purpose, refer to the other analysis notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe6dbf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter any columns containing the \"current\" keyword\n",
    "### Filter again with \"annual keywords\"\n",
    "### Join with the fire_dataset (points) to get back the years / month of fire occurence \n",
    "merged_df = df.filter(regex=r'current')\n",
    "merged_df = merged_df.filter(regex=r'annual')\n",
    "merged_df = merged_df.join(fire_dataset, lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f89b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop all columns containing only NaN values\n",
    "merged_df = merged_df.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fd784cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('combined_data_filtered_current_annual_nan.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aebaf07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_aet_annual</th>\n",
       "      <th>current_def_annual</th>\n",
       "      <th>current_pdsi_annual</th>\n",
       "      <th>current_pet_annual</th>\n",
       "      <th>current_pr_annual</th>\n",
       "      <th>current_ro_annual</th>\n",
       "      <th>current_soil_annual</th>\n",
       "      <th>current_srad_annual</th>\n",
       "      <th>current_swe_annual</th>\n",
       "      <th>current_tmmn_annual</th>\n",
       "      <th>...</th>\n",
       "      <th>ADM0_EN</th>\n",
       "      <th>ADM1_EN</th>\n",
       "      <th>ADM2_EN</th>\n",
       "      <th>validOn</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>ADM0_PCODE</th>\n",
       "      <th>BurnDate</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system:index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_2001_09_01_00000000000000000071_0</th>\n",
       "      <td>107.900000</td>\n",
       "      <td>1.591667</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>109.483333</td>\n",
       "      <td>296.416667</td>\n",
       "      <td>188.666667</td>\n",
       "      <td>111.983333</td>\n",
       "      <td>188.691667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Johor</td>\n",
       "      <td>Segamat</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.233776</td>\n",
       "      <td>MY</td>\n",
       "      <td>270</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_2001_09_01_00000000000000000071_1</th>\n",
       "      <td>107.900000</td>\n",
       "      <td>1.591667</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>109.483333</td>\n",
       "      <td>296.416667</td>\n",
       "      <td>188.666667</td>\n",
       "      <td>111.983333</td>\n",
       "      <td>188.691667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Johor</td>\n",
       "      <td>Segamat</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.233776</td>\n",
       "      <td>MY</td>\n",
       "      <td>256</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_2001_09_01_00000000000000000014_0</th>\n",
       "      <td>107.300000</td>\n",
       "      <td>1.483333</td>\n",
       "      <td>1.194167</td>\n",
       "      <td>108.791667</td>\n",
       "      <td>294.416667</td>\n",
       "      <td>187.166667</td>\n",
       "      <td>111.141667</td>\n",
       "      <td>187.225000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.541667</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Terengganu</td>\n",
       "      <td>Dungun</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.219613</td>\n",
       "      <td>MY</td>\n",
       "      <td>264</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_2001_09_01_00000000000000000014_1</th>\n",
       "      <td>107.300000</td>\n",
       "      <td>1.483333</td>\n",
       "      <td>1.194167</td>\n",
       "      <td>108.791667</td>\n",
       "      <td>294.416667</td>\n",
       "      <td>187.166667</td>\n",
       "      <td>111.141667</td>\n",
       "      <td>187.225000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.541667</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Terengganu</td>\n",
       "      <td>Dungun</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.219613</td>\n",
       "      <td>MY</td>\n",
       "      <td>264</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_2001_09_01_00000000000000000014_2</th>\n",
       "      <td>100.983333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.966667</td>\n",
       "      <td>101.900000</td>\n",
       "      <td>204.416667</td>\n",
       "      <td>103.333333</td>\n",
       "      <td>86.308333</td>\n",
       "      <td>173.558333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.841667</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Terengganu</td>\n",
       "      <td>Dungun</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.219613</td>\n",
       "      <td>MY</td>\n",
       "      <td>257</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_5645</th>\n",
       "      <td>95.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.268333</td>\n",
       "      <td>95.875000</td>\n",
       "      <td>337.166667</td>\n",
       "      <td>241.250000</td>\n",
       "      <td>107.600000</td>\n",
       "      <td>170.191667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Terengganu</td>\n",
       "      <td>Hulu Terengganu</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.316419</td>\n",
       "      <td>MY</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_5646</th>\n",
       "      <td>97.558333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.896667</td>\n",
       "      <td>97.558333</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>96.700000</td>\n",
       "      <td>168.191667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Negeri</td>\n",
       "      <td>Jelebu</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.110271</td>\n",
       "      <td>MY</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_5647</th>\n",
       "      <td>85.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.253333</td>\n",
       "      <td>85.125000</td>\n",
       "      <td>334.166667</td>\n",
       "      <td>249.166667</td>\n",
       "      <td>113.700000</td>\n",
       "      <td>168.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.866667</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Pahang</td>\n",
       "      <td>Raub</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.186307</td>\n",
       "      <td>MY</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_5648</th>\n",
       "      <td>99.191667</td>\n",
       "      <td>2.025000</td>\n",
       "      <td>4.381667</td>\n",
       "      <td>101.216667</td>\n",
       "      <td>209.583333</td>\n",
       "      <td>110.500000</td>\n",
       "      <td>61.966667</td>\n",
       "      <td>165.216667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Johor</td>\n",
       "      <td>Batu Pahat</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.161041</td>\n",
       "      <td>MY</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_5649</th>\n",
       "      <td>98.508333</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>99.458333</td>\n",
       "      <td>255.416667</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>87.541667</td>\n",
       "      <td>165.441667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Terengganu</td>\n",
       "      <td>Dungun</td>\n",
       "      <td>1613030400000</td>\n",
       "      <td>0.219613</td>\n",
       "      <td>MY</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11279 rows Ã 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     current_aet_annual  current_def_annual  \\\n",
       "system:index                                                                  \n",
       "1_2001_09_01_00000000000000000071_0          107.900000            1.591667   \n",
       "1_2001_09_01_00000000000000000071_1          107.900000            1.591667   \n",
       "1_2001_09_01_00000000000000000014_0          107.300000            1.483333   \n",
       "1_2001_09_01_00000000000000000014_1          107.300000            1.483333   \n",
       "1_2001_09_01_00000000000000000014_2          100.983333            0.916667   \n",
       "...                                                 ...                 ...   \n",
       "2_5645                                        95.875000            0.000000   \n",
       "2_5646                                        97.558333            0.000000   \n",
       "2_5647                                        85.125000            0.000000   \n",
       "2_5648                                        99.191667            2.025000   \n",
       "2_5649                                        98.508333            0.950000   \n",
       "\n",
       "                                     current_pdsi_annual  current_pet_annual  \\\n",
       "system:index                                                                   \n",
       "1_2001_09_01_00000000000000000071_0             1.225000          109.483333   \n",
       "1_2001_09_01_00000000000000000071_1             1.225000          109.483333   \n",
       "1_2001_09_01_00000000000000000014_0             1.194167          108.791667   \n",
       "1_2001_09_01_00000000000000000014_1             1.194167          108.791667   \n",
       "1_2001_09_01_00000000000000000014_2             1.966667          101.900000   \n",
       "...                                                  ...                 ...   \n",
       "2_5645                                          4.268333           95.875000   \n",
       "2_5646                                          1.896667           97.558333   \n",
       "2_5647                                          3.253333           85.125000   \n",
       "2_5648                                          4.381667          101.216667   \n",
       "2_5649                                          1.400000           99.458333   \n",
       "\n",
       "                                     current_pr_annual  current_ro_annual  \\\n",
       "system:index                                                                \n",
       "1_2001_09_01_00000000000000000071_0         296.416667         188.666667   \n",
       "1_2001_09_01_00000000000000000071_1         296.416667         188.666667   \n",
       "1_2001_09_01_00000000000000000014_0         294.416667         187.166667   \n",
       "1_2001_09_01_00000000000000000014_1         294.416667         187.166667   \n",
       "1_2001_09_01_00000000000000000014_2         204.416667         103.333333   \n",
       "...                                                ...                ...   \n",
       "2_5645                                      337.166667         241.250000   \n",
       "2_5646                                      250.000000         152.500000   \n",
       "2_5647                                      334.166667         249.166667   \n",
       "2_5648                                      209.583333         110.500000   \n",
       "2_5649                                      255.416667         157.000000   \n",
       "\n",
       "                                     current_soil_annual  current_srad_annual  \\\n",
       "system:index                                                                    \n",
       "1_2001_09_01_00000000000000000071_0           111.983333           188.691667   \n",
       "1_2001_09_01_00000000000000000071_1           111.983333           188.691667   \n",
       "1_2001_09_01_00000000000000000014_0           111.141667           187.225000   \n",
       "1_2001_09_01_00000000000000000014_1           111.141667           187.225000   \n",
       "1_2001_09_01_00000000000000000014_2            86.308333           173.558333   \n",
       "...                                                  ...                  ...   \n",
       "2_5645                                        107.600000           170.191667   \n",
       "2_5646                                         96.700000           168.191667   \n",
       "2_5647                                        113.700000           168.666667   \n",
       "2_5648                                         61.966667           165.216667   \n",
       "2_5649                                         87.541667           165.441667   \n",
       "\n",
       "                                     current_swe_annual  current_tmmn_annual  \\\n",
       "system:index                                                                   \n",
       "1_2001_09_01_00000000000000000071_0                 0.0            23.625000   \n",
       "1_2001_09_01_00000000000000000071_1                 0.0            23.625000   \n",
       "1_2001_09_01_00000000000000000014_0                 0.0            23.541667   \n",
       "1_2001_09_01_00000000000000000014_1                 0.0            23.541667   \n",
       "1_2001_09_01_00000000000000000014_2                 0.0            22.841667   \n",
       "...                                                 ...                  ...   \n",
       "2_5645                                              0.0            21.450000   \n",
       "2_5646                                              0.0            21.666667   \n",
       "2_5647                                              0.0            17.866667   \n",
       "2_5648                                              0.0            23.050000   \n",
       "2_5649                                              0.0            22.425000   \n",
       "\n",
       "                                     ...   ADM0_EN     ADM1_EN  \\\n",
       "system:index                         ...                         \n",
       "1_2001_09_01_00000000000000000071_0  ...  Malaysia       Johor   \n",
       "1_2001_09_01_00000000000000000071_1  ...  Malaysia       Johor   \n",
       "1_2001_09_01_00000000000000000014_0  ...  Malaysia  Terengganu   \n",
       "1_2001_09_01_00000000000000000014_1  ...  Malaysia  Terengganu   \n",
       "1_2001_09_01_00000000000000000014_2  ...  Malaysia  Terengganu   \n",
       "...                                  ...       ...         ...   \n",
       "2_5645                               ...  Malaysia  Terengganu   \n",
       "2_5646                               ...  Malaysia      Negeri   \n",
       "2_5647                               ...  Malaysia      Pahang   \n",
       "2_5648                               ...  Malaysia       Johor   \n",
       "2_5649                               ...  Malaysia  Terengganu   \n",
       "\n",
       "                                             ADM2_EN        validOn  \\\n",
       "system:index                                                          \n",
       "1_2001_09_01_00000000000000000071_0          Segamat  1613030400000   \n",
       "1_2001_09_01_00000000000000000071_1          Segamat  1613030400000   \n",
       "1_2001_09_01_00000000000000000014_0           Dungun  1613030400000   \n",
       "1_2001_09_01_00000000000000000014_1           Dungun  1613030400000   \n",
       "1_2001_09_01_00000000000000000014_2           Dungun  1613030400000   \n",
       "...                                              ...            ...   \n",
       "2_5645                               Hulu Terengganu  1613030400000   \n",
       "2_5646                                        Jelebu  1613030400000   \n",
       "2_5647                                          Raub  1613030400000   \n",
       "2_5648                                    Batu Pahat  1613030400000   \n",
       "2_5649                                        Dungun  1613030400000   \n",
       "\n",
       "                                     Shape_Area  ADM0_PCODE  BurnDate  year  \\\n",
       "system:index                                                                  \n",
       "1_2001_09_01_00000000000000000071_0    0.233776          MY       270  2001   \n",
       "1_2001_09_01_00000000000000000071_1    0.233776          MY       256  2001   \n",
       "1_2001_09_01_00000000000000000014_0    0.219613          MY       264  2001   \n",
       "1_2001_09_01_00000000000000000014_1    0.219613          MY       264  2001   \n",
       "1_2001_09_01_00000000000000000014_2    0.219613          MY       257  2001   \n",
       "...                                         ...         ...       ...   ...   \n",
       "2_5645                                 0.316419          MY        -1  2023   \n",
       "2_5646                                 0.110271          MY        -1  2023   \n",
       "2_5647                                 0.186307          MY        -1  2023   \n",
       "2_5648                                 0.161041          MY        -1  2023   \n",
       "2_5649                                 0.219613          MY        -1  2023   \n",
       "\n",
       "                                     month day  \n",
       "system:index                                    \n",
       "1_2001_09_01_00000000000000000071_0      9  27  \n",
       "1_2001_09_01_00000000000000000071_1      9  13  \n",
       "1_2001_09_01_00000000000000000014_0      9  21  \n",
       "1_2001_09_01_00000000000000000014_1      9  21  \n",
       "1_2001_09_01_00000000000000000014_2      9  14  \n",
       "...                                    ...  ..  \n",
       "2_5645                                  12  31  \n",
       "2_5646                                  12  31  \n",
       "2_5647                                  12  31  \n",
       "2_5648                                  12  31  \n",
       "2_5649                                  12  31  \n",
       "\n",
       "[11279 rows x 39 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272.953px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
